{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Import necessary PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([#transforms.ToPILImage(),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       #transforms.RandomVerticalFlip(),\n",
    "                                       #transforms.RandomRotation(45),\n",
    "                                       #transforms.RandomAffine(45),\n",
    "                                       transforms.ColorJitter(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Use 10-crop for Test Time Augmentation\n",
    "test_transforms = transforms.Compose([#transforms.ToPILImage(),\n",
    "                                      transforms.Resize(256),\n",
    "                                      transforms.RandomResizedCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(\"D:/kood\", transform=train_transforms)\n",
    "#D:\\food\\testdata\n",
    "test_data = datasets.ImageFolder(\"D:/food/testdata\", transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size, shuffle=True)\n",
    "val_dl = DataLoader(test_data, batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first batch of training images:  torch.Size([16, 3, 224, 224])\n",
      "Shape of first batch of test images:  torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Analyzing the shape of one batch\n",
    "train_images, train_labels = next(iter(train_dl))\n",
    "test_images, test_labels = next(iter(val_dl))\n",
    "\n",
    "print('Shape of first batch of training images: ', train_images.shape)\n",
    "print('Shape of first batch of test images: ', test_images.shape) # 5-D array because of 10 crops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 197076 images under train\n",
      "Loaded 1563 images under val\n",
      "Classes: \n",
      "['가지볶음', '간장게장', '갈비구이', '갈비찜', '갈비탕', '갈치구이', '갈치조림', '감자전', '감자조림', '감자채볶음', '감자탕', '갓김치', '건새우볶음', '경단', '계란국', '계란말이', '계란찜', '계란후라이', '고등어구이', '고등어조림', '고사리나물', '고추장진미채볶음', '고추튀김', '곰탕_설렁탕', '곱창구이', '곱창전골', '과메기', '굴', '김밥', '김치볶음밥', '김치전', '김치찌개', '김치찜', '깍두기', '깻잎장아찌', '꼬막찜', '꽁치조림', '꽈리고추무침', '꿀떡', '나박김치', '나초', '누룽지', '닭갈비', '닭계장', '닭볶음탕', '당근케이크', '더덕구이', '도넛', '도라지무침', '도토리묵', '돈까스tmp', '동그랑땡', '동태찌개', '돼지고기', '된장국', '된장찌개', '두부김치', '두부조림', '디저트', '딸기케이크', '땅콩조림', '떡갈비', '떡국_만두국', '떡꼬치', '떡볶이', '라면', '라볶이', '레드케이크', '리조또', '마늘빵', '마카롱', '막국수', '만두', '매운탕', '멍게', '메추리알장조림', '멸치볶음', '무국', '무생채', '물냉면', '물회', '미역국', '미역줄기볶음', '배추김치', '백김치', '보쌈', '부추김치', '북엇국', '불고기', '브리또', '비빔냉면', '비빔밥', '산낙지', '삼겹살', '삼계탕', '새우볶음밥', '새우튀김', '샌드위치', '샐러드', '생선전', '소세지볶음', '송편', '수육', '수정과', '수제비', '숙주나물', '순대', '순두부찌개', '스테이크', '스테이크2', '스프', '시금치나물', '시래기국', '식혜', '쌀밥tmp', '아이스크림', '알밥', '애플파이', '애호박볶음', '약과', '약식', '양념게장', '양념치킨', '양파링', '어묵볶음', '연근조림', '연어요리', '열무국수', '열무김치', '오믈렛', '오이소박이', '오징어채볶음', '오징어튀김', '와플', '우엉조림', '유부초밥', '육개장', '육회', '잔치국수', '잡곡밥', '잡채', '장어구이', '장조림', '전복죽', '젓갈', '제육볶음', '조개구이', '조기구이', '족발', '주꾸미볶음', '주먹밥', '짜장면', '짬뽕', '쫄면', '찐만두', '찜닭', '초밥', '초콜릿케이크', '총각김치', '추어탕', '츄러스', '치즈케이크', '치킨윙', '치킨카레', '칼국수', '컵케이크', '코다리조림', '콩국수', '콩나물국', '콩나물무침', '콩자반', '크림스파게티', '타꼬', '타꼬야끼', '탕수육tmp', '토마토스파게티', '토스트', '티라미슈', '파김치', '파전', '팟타이', '팬케이크', '편육', '포', '피자', '한과', '핫도그', '해물찜', '해조류무침', '햄버거', '현미밥tmp', '호박전', '호박죽', '홍어무침', '황태구이', '회', '회무침', '후라이드치킨', '훈제오리']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loaded {} images under {}\".format(len(train_data), \"train\"))\n",
    "print(\"Loaded {} images under {}\".format(len(test_data), \"val\"))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "print(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"가지볶음\",\n",
      "\"간장게장\",\n",
      "\"갈비구이\",\n",
      "\"갈비찜\",\n",
      "\"갈비탕\",\n",
      "\"갈치구이\",\n",
      "\"갈치조림\",\n",
      "\"감자전\",\n",
      "\"감자조림\",\n",
      "\"감자채볶음\",\n",
      "\"감자탕\",\n",
      "\"갓김치\",\n",
      "\"건새우볶음\",\n",
      "\"경단\",\n",
      "\"계란국\",\n",
      "\"계란말이\",\n",
      "\"계란찜\",\n",
      "\"계란후라이\",\n",
      "\"고등어구이\",\n",
      "\"고등어조림\",\n",
      "\"고사리나물\",\n",
      "\"고추장진미채볶음\",\n",
      "\"고추튀김\",\n",
      "\"곰탕_설렁탕\",\n",
      "\"곱창구이\",\n",
      "\"곱창전골\",\n",
      "\"과메기\",\n",
      "\"굴\",\n",
      "\"김밥\",\n",
      "\"김치볶음밥\",\n",
      "\"김치전\",\n",
      "\"김치찌개\",\n",
      "\"김치찜\",\n",
      "\"깍두기\",\n",
      "\"깻잎장아찌\",\n",
      "\"꼬막찜\",\n",
      "\"꽁치조림\",\n",
      "\"꽈리고추무침\",\n",
      "\"꿀떡\",\n",
      "\"나박김치\",\n",
      "\"나초\",\n",
      "\"누룽지\",\n",
      "\"닭갈비\",\n",
      "\"닭계장\",\n",
      "\"닭볶음탕\",\n",
      "\"당근케이크\",\n",
      "\"더덕구이\",\n",
      "\"도넛\",\n",
      "\"도라지무침\",\n",
      "\"도토리묵\",\n",
      "\"돈까스tmp\",\n",
      "\"동그랑땡\",\n",
      "\"동태찌개\",\n",
      "\"돼지고기\",\n",
      "\"된장국\",\n",
      "\"된장찌개\",\n",
      "\"두부김치\",\n",
      "\"두부조림\",\n",
      "\"디저트\",\n",
      "\"딸기케이크\",\n",
      "\"땅콩조림\",\n",
      "\"떡갈비\",\n",
      "\"떡국_만두국\",\n",
      "\"떡꼬치\",\n",
      "\"떡볶이\",\n",
      "\"라면\",\n",
      "\"라볶이\",\n",
      "\"레드케이크\",\n",
      "\"리조또\",\n",
      "\"마늘빵\",\n",
      "\"마카롱\",\n",
      "\"막국수\",\n",
      "\"만두\",\n",
      "\"매운탕\",\n",
      "\"멍게\",\n",
      "\"메추리알장조림\",\n",
      "\"멸치볶음\",\n",
      "\"무국\",\n",
      "\"무생채\",\n",
      "\"물냉면\",\n",
      "\"물회\",\n",
      "\"미역국\",\n",
      "\"미역줄기볶음\",\n",
      "\"배추김치\",\n",
      "\"백김치\",\n",
      "\"보쌈\",\n",
      "\"부추김치\",\n",
      "\"북엇국\",\n",
      "\"불고기\",\n",
      "\"브리또\",\n",
      "\"비빔냉면\",\n",
      "\"비빔밥\",\n",
      "\"산낙지\",\n",
      "\"삼겹살\",\n",
      "\"삼계탕\",\n",
      "\"새우볶음밥\",\n",
      "\"새우튀김\",\n",
      "\"샌드위치\",\n",
      "\"샐러드\",\n",
      "\"생선전\",\n",
      "\"소세지볶음\",\n",
      "\"송편\",\n",
      "\"수육\",\n",
      "\"수정과\",\n",
      "\"수제비\",\n",
      "\"숙주나물\",\n",
      "\"순대\",\n",
      "\"순두부찌개\",\n",
      "\"스테이크\",\n",
      "\"스테이크2\",\n",
      "\"스프\",\n",
      "\"시금치나물\",\n",
      "\"시래기국\",\n",
      "\"식혜\",\n",
      "\"쌀밥tmp\",\n",
      "\"아이스크림\",\n",
      "\"알밥\",\n",
      "\"애플파이\",\n",
      "\"애호박볶음\",\n",
      "\"약과\",\n",
      "\"약식\",\n",
      "\"양념게장\",\n",
      "\"양념치킨\",\n",
      "\"양파링\",\n",
      "\"어묵볶음\",\n",
      "\"연근조림\",\n",
      "\"연어요리\",\n",
      "\"열무국수\",\n",
      "\"열무김치\",\n",
      "\"오믈렛\",\n",
      "\"오이소박이\",\n",
      "\"오징어채볶음\",\n",
      "\"오징어튀김\",\n",
      "\"와플\",\n",
      "\"우엉조림\",\n",
      "\"유부초밥\",\n",
      "\"육개장\",\n",
      "\"육회\",\n",
      "\"잔치국수\",\n",
      "\"잡곡밥\",\n",
      "\"잡채\",\n",
      "\"장어구이\",\n",
      "\"장조림\",\n",
      "\"전복죽\",\n",
      "\"젓갈\",\n",
      "\"제육볶음\",\n",
      "\"조개구이\",\n",
      "\"조기구이\",\n",
      "\"족발\",\n",
      "\"주꾸미볶음\",\n",
      "\"주먹밥\",\n",
      "\"짜장면\",\n",
      "\"짬뽕\",\n",
      "\"쫄면\",\n",
      "\"찐만두\",\n",
      "\"찜닭\",\n",
      "\"초밥\",\n",
      "\"초콜릿케이크\",\n",
      "\"총각김치\",\n",
      "\"추어탕\",\n",
      "\"츄러스\",\n",
      "\"치즈케이크\",\n",
      "\"치킨윙\",\n",
      "\"치킨카레\",\n",
      "\"칼국수\",\n",
      "\"컵케이크\",\n",
      "\"코다리조림\",\n",
      "\"콩국수\",\n",
      "\"콩나물국\",\n",
      "\"콩나물무침\",\n",
      "\"콩자반\",\n",
      "\"크림스파게티\",\n",
      "\"타꼬\",\n",
      "\"타꼬야끼\",\n",
      "\"탕수육tmp\",\n",
      "\"토마토스파게티\",\n",
      "\"토스트\",\n",
      "\"티라미슈\",\n",
      "\"파김치\",\n",
      "\"파전\",\n",
      "\"팟타이\",\n",
      "\"팬케이크\",\n",
      "\"편육\",\n",
      "\"포\",\n",
      "\"피자\",\n",
      "\"한과\",\n",
      "\"핫도그\",\n",
      "\"해물찜\",\n",
      "\"해조류무침\",\n",
      "\"햄버거\",\n",
      "\"현미밥tmp\",\n",
      "\"호박전\",\n",
      "\"호박죽\",\n",
      "\"홍어무침\",\n",
      "\"황태구이\",\n",
      "\"회\",\n",
      "\"회무침\",\n",
      "\"후라이드치킨\",\n",
      "\"훈제오리\",\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "path_dir = 'D:\\kood'\n",
    " \n",
    "file_list = os.listdir(path_dir)\n",
    "for i in file_list:\n",
    "    print('\"'+i+'\",')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet101(pretrained=True)            \n",
    "\n",
    "number_of_features =  model.fc.in_features\n",
    "model.fc = nn.Linear(number_of_features, 199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=199, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer same as used earlier\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# secify scheduler same as used earlier\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min', patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197076\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataset_sizes = {}\n",
    "dataloaders['train'] = train_dl\n",
    "dataloaders['val'] = val_dl\n",
    "dataset_sizes['train'] = len(train_dl.dataset)\n",
    "dataset_sizes['val'] = len(val_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:               \n",
    "                continue\n",
    "                #model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in tqdm(dataloaders[phase]):\n",
    "                x, y = data;\n",
    "                inputs = x.squeeze(0).to(device)\n",
    "                labels = y.to(device)\n",
    "#                 if phase == 'train':\n",
    "#                   inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, 0.4, device)\n",
    "#                   inputs, targets_a, targets_b = Variable(inputs), Variable(labels_a), Variable(labels_b)\n",
    "                  \n",
    "                inputs = inputs.to(device)\n",
    "#                 if phase == 'train':\n",
    "#                   labels_a = labels_a.to(device)\n",
    "#                   labels_b = labels_b.to(device)\n",
    "#                 else:\n",
    "#                   labels = labels.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "#                     loss_func = mixup_criterion(labels_a,labels_b,lam)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "#                     if phase=='train':\n",
    "#                       loss = loss_func(criterion,outputs)\n",
    "#                     else:\n",
    "#                         loss = criterion(outputs,labels) \n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "#                 if phase == 'train':\n",
    "#                   running_corrects += lam * preds.eq(labels_a.data).sum() + (1 - lam) * preds.eq(labels_b.data).sum()\n",
    "#                 else:\n",
    "#                     running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)            \n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            torch.save(model.state_dict(), '10_12_test_199_1.pth')\n",
    "            # deep copy the model\n",
    "            #if phase == 'val':\n",
    "            #    best_acc = epoch_acc\n",
    "            #    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed0ba25b45640cba210add0c4875eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4784 Acc: 0.8666\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf5dec6a2384bf993a246c1c7a74ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4763 Acc: 0.8683\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67112bed1944894a98fe8851e0dba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4781 Acc: 0.8667\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d4dc2c4bab46c0847f1d60f0f88add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4753 Acc: 0.8683\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f264c55de0fb4dc8b4367fb3f841356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4790 Acc: 0.8672\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46722bca85a409dbc92b8b33633f4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4771 Acc: 0.8672\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ce96b08c2749a5808bfb3bb7d82589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4763 Acc: 0.8673\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ce993872ef43008ffccff358325d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4770 Acc: 0.8677\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db5d00468594c51a350511e6707086c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4774 Acc: 0.8677\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8193b4b24d3045fcbc8b2bac4061cc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4774 Acc: 0.8676\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19208c5bd1f5486aadf26d7af27336bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4781 Acc: 0.8665\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89e4133169b4c7cbe081271e9f2d424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4774 Acc: 0.8673\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fbd5cdcb344e228ef30a16cf67129f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4778 Acc: 0.8664\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a63f06dca34f4cb12735c1d49c4df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4735 Acc: 0.8680\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846e14959e4649af905ebc96c6d692be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4755 Acc: 0.8682\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1332856a645d442eb748ba5532f40ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4784 Acc: 0.8669\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a0925917b844d6b6d1fde8163634ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4791 Acc: 0.8660\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85bdc79368c46b0a95d64daff7670e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4783 Acc: 0.8672\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957ee8ad15c24109adfcda6ce6859054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4737 Acc: 0.8683\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baea28f8eca84f659b0b67b65fb3226a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4803 Acc: 0.8664\n",
      "\n",
      "Training complete in 3143m 6s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet101()            \n",
    "\n",
    "number_of_features =  model.fc.in_features\n",
    "model.fc = nn.Linear(number_of_features, 199)\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "    \n",
    "model.load_state_dict(torch.load(\"10_12_test_199_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = models.resnet101()            \n",
    "\n",
    "number_of_features =  model1.fc.in_features\n",
    "model1.fc = nn.Linear(number_of_features, 199)\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    model1 = model1.cuda()\n",
    "    \n",
    "model1.load_state_dict(torch.load(\"10_12_test_199_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms1 = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.TenCrop(224),\n",
    "                                      transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                      transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(crop) for crop in crops]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(\"D:/food/testdata\", transform=test_transforms1)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size= 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "    \n",
    "with open('D:/food/food-101/meta/class.txt', 'r',encoding='UTF8') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "#classes = ['apple_pie', 'baby_back_ribs', 'baklava']\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "batch_time = AverageMeter('Time', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.4e')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "valid_acc_list = []\n",
    "#move model to gpu\n",
    "model1.cuda()\n",
    "model1.eval()\n",
    "count =0\n",
    "# iterate over test data\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if USE_GPU:\n",
    "      data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "      ## For 10-crop Testing\n",
    "      bs, ncrops, c, h, w = data.size()\n",
    "      # forward pass: compute predicted outputs by passing inputs to the model\n",
    "      temp_output = model1(data.view(-1, c, h, w))\n",
    "      output = temp_output.view(bs, ncrops, -1).mean(1)\n",
    "      # calculate the batch loss\n",
    "      loss = criterion(output, target)\n",
    "      \n",
    "      acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "      print()\n",
    "      #valid_acc_list.append(prec1.data/100)\n",
    "      \n",
    "\n",
    "    # update average test loss \n",
    "      test_loss += loss.item()*data.size(0)\n",
    "      # convert output probabilities to predicted class\n",
    "      top1.update(acc1[0], data.size(0))\n",
    "        \n",
    "      top5.update(acc5[0], data.size(0))\n",
    "    \n",
    "      _, pred = torch.max(output, 1)    \n",
    "      # compare predictions to true label\n",
    "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "      correct = np.squeeze(correct_tensor.numpy()) if not USE_GPU else np.squeeze(correct_tensor.cpu().numpy())\n",
    "      # calculate test accuracy for each object class\n",
    "      for i in range(len(target)):\n",
    "          label = target.data[i]\n",
    "          class_correct[label] += correct[i].item()\n",
    "          class_total[label] += 1\n",
    "            \n",
    "print(top5.avg)    \n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %.2f%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],\n",
    "                                                         np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),\n",
    "                                                      np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(99.7098, device='cuda:0')\n",
      "Test Loss: 0.157293\n",
      "\n",
      "Test Accuracy of  가지볶음: 97.50% (975/1000)\n",
      "Test Accuracy of  간장게장: 97.80% (979/1001)\n",
      "Test Accuracy of  갈비구이: 94.50% (945/1000)\n",
      "Test Accuracy of   갈비찜: 88.80% (888/1000)\n",
      "Test Accuracy of   갈비탕: 92.30% (923/1000)\n",
      "Test Accuracy of  갈치구이: 97.80% (978/1000)\n",
      "Test Accuracy of  갈치조림: 89.45% (899/1005)\n",
      "Test Accuracy of   감자전: 95.10% (952/1001)\n",
      "Test Accuracy of  감자조림: 97.90% (980/1001)\n",
      "Test Accuracy of 감자채볶음: 99.80% (998/1000)\n",
      "Test Accuracy of   감자탕: 93.02% (933/1003)\n",
      "Test Accuracy of   갓김치: 86.43% (860/995)\n",
      "Test Accuracy of 건새우볶음: 98.50% (985/1000)\n",
      "Test Accuracy of    경단: 99.70% (997/1000)\n",
      "Test Accuracy of   계란국: 96.00% (961/1001)\n",
      "Test Accuracy of  계란말이: 99.60% (997/1001)\n",
      "Test Accuracy of   계란찜: 98.40% (987/1003)\n",
      "Test Accuracy of 계란후라이: 93.31% (935/1002)\n",
      "Test Accuracy of 고등어구이: 94.50% (945/1000)\n",
      "Test Accuracy of 고등어조림: 87.20% (872/1000)\n",
      "Test Accuracy of 고사리나물: 99.20% (992/1000)\n",
      "Test Accuracy of 고추장진미채볶음: 95.30% (953/1000)\n",
      "Test Accuracy of  고추튀김: 97.12% (978/1007)\n",
      "Test Accuracy of 곰탕_설렁탕: 90.12% (903/1002)\n",
      "Test Accuracy of  곱창구이: 99.30% (993/1000)\n",
      "Test Accuracy of  곱창전골: 94.60% (946/1000)\n",
      "Test Accuracy of   과메기: 99.70% (1000/1003)\n",
      "Test Accuracy of     굴: 97.50% (975/1000)\n",
      "Test Accuracy of    김밥: 98.50% (984/999)\n",
      "Test Accuracy of 김치볶음밥: 97.70% (976/999)\n",
      "Test Accuracy of   김치전: 93.81% (939/1001)\n",
      "Test Accuracy of  김치찌개: 88.52% (887/1002)\n",
      "Test Accuracy of   김치찜: 94.59% (944/998)\n",
      "Test Accuracy of   깍두기: 97.90% (979/1000)\n",
      "Test Accuracy of 깻잎장아찌: 97.90% (980/1001)\n",
      "Test Accuracy of   꼬막찜: 99.20% (996/1004)\n",
      "Test Accuracy of  꽁치조림: 93.49% (934/999)\n",
      "Test Accuracy of 꽈리고추무침: 95.79% (956/998)\n",
      "Test Accuracy of    꿀떡: 97.50% (976/1001)\n",
      "Test Accuracy of  나박김치: 98.70% (986/999)\n",
      "Test Accuracy of    나초: 98.40% (984/1000)\n",
      "Test Accuracy of   누룽지: 95.70% (957/1000)\n",
      "Test Accuracy of   닭갈비: 93.20% (932/1000)\n",
      "Test Accuracy of   닭계장: 84.65% (849/1003)\n",
      "Test Accuracy of  닭볶음탕: 89.52% (897/1002)\n",
      "Test Accuracy of 당근케이크: 95.90% (959/1000)\n",
      "Test Accuracy of  더덕구이: 94.50% (945/1000)\n",
      "Test Accuracy of    도넛: 98.00% (980/1000)\n",
      "Test Accuracy of 도라지무침: 93.30% (933/1000)\n",
      "Test Accuracy of  도토리묵: 98.70% (989/1002)\n",
      "Test Accuracy of 돈까스tmp: 96.63% (544/563)\n",
      "Test Accuracy of  동그랑땡: 96.80% (968/1000)\n",
      "Test Accuracy of  동태찌개: 86.40% (864/1000)\n",
      "Test Accuracy of  돼지고기: 75.00% (750/1000)\n",
      "Test Accuracy of   된장국: 99.40% (994/1000)\n",
      "Test Accuracy of  된장찌개: 95.60% (955/999)\n",
      "Test Accuracy of  두부김치: 99.70% (997/1000)\n",
      "Test Accuracy of  두부조림: 95.90% (959/1000)\n",
      "Test Accuracy of   디저트: 93.60% (936/1000)\n",
      "Test Accuracy of 딸기케이크: 96.00% (960/1000)\n",
      "Test Accuracy of  땅콩조림: 98.50% (986/1001)\n",
      "Test Accuracy of   떡갈비: 95.50% (955/1000)\n",
      "Test Accuracy of 떡국_만두국: 94.18% (939/997)\n",
      "Test Accuracy of   떡꼬치: 99.36% (1088/1095)\n",
      "Test Accuracy of   떡볶이: 90.36% (900/996)\n",
      "Test Accuracy of    라면: 94.28% (940/997)\n",
      "Test Accuracy of   라볶이: 99.50% (995/1000)\n",
      "Test Accuracy of 레드케이크: 92.80% (928/1000)\n",
      "Test Accuracy of   리조또: 94.30% (943/1000)\n",
      "Test Accuracy of   마늘빵: 97.10% (971/1000)\n",
      "Test Accuracy of   마카롱: 98.70% (987/1000)\n",
      "Test Accuracy of   막국수: 99.00% (986/996)\n",
      "Test Accuracy of    만두: 98.50% (985/1000)\n",
      "Test Accuracy of   매운탕: 84.92% (850/1001)\n",
      "Test Accuracy of    멍게: 96.42% (1050/1089)\n",
      "Test Accuracy of 메추리알장조림: 86.21% (863/1001)\n",
      "Test Accuracy of  멸치볶음: 98.10% (980/999)\n",
      "Test Accuracy of    무국: 97.90% (979/1000)\n",
      "Test Accuracy of   무생채: 95.70% (957/1000)\n",
      "Test Accuracy of   물냉면: 88.23% (877/994)\n",
      "Test Accuracy of    물회: 98.22% (995/1013)\n",
      "Test Accuracy of   미역국: 99.50% (994/999)\n",
      "Test Accuracy of 미역줄기볶음: 99.70% (997/1000)\n",
      "Test Accuracy of  배추김치: 98.30% (983/1000)\n",
      "Test Accuracy of   백김치: 99.40% (992/998)\n",
      "Test Accuracy of    보쌈: 94.41% (945/1001)\n",
      "Test Accuracy of  부추김치: 94.58% (943/997)\n",
      "Test Accuracy of   북엇국: 95.80% (958/1000)\n",
      "Test Accuracy of   불고기: 93.10% (931/1000)\n",
      "Test Accuracy of   브리또: 96.10% (961/1000)\n",
      "Test Accuracy of  비빔냉면: 89.93% (902/1003)\n",
      "Test Accuracy of   비빔밥: 94.08% (937/996)\n",
      "Test Accuracy of   산낙지: 99.15% (1056/1065)\n",
      "Test Accuracy of   삼겹살: 90.04% (895/994)\n",
      "Test Accuracy of   삼계탕: 99.70% (1000/1003)\n",
      "Test Accuracy of 새우볶음밥: 99.10% (991/1000)\n",
      "Test Accuracy of  새우튀김: 94.32% (946/1003)\n",
      "Test Accuracy of  샌드위치: 95.60% (956/1000)\n",
      "Test Accuracy of   샐러드: 98.30% (983/1000)\n",
      "Test Accuracy of   생선전: 96.90% (969/1000)\n",
      "Test Accuracy of 소세지볶음: 97.01% (973/1003)\n",
      "Test Accuracy of    송편: 99.40% (994/1000)\n",
      "Test Accuracy of    수육: 84.95% (852/1003)\n",
      "Test Accuracy of   수정과: 99.40% (993/999)\n",
      "Test Accuracy of   수제비: 89.77% (895/997)\n",
      "Test Accuracy of  숙주나물: 99.20% (992/1000)\n",
      "Test Accuracy of    순대: 99.10% (994/1003)\n",
      "Test Accuracy of 순두부찌개: 95.80% (958/1000)\n",
      "Test Accuracy of  스테이크: 88.80% (888/1000)\n",
      "Test Accuracy of 스테이크2: 85.70% (857/1000)\n",
      "Test Accuracy of    스프: 98.00% (980/1000)\n",
      "Test Accuracy of 시금치나물: 99.60% (996/1000)\n",
      "Test Accuracy of  시래기국: 96.90% (969/1000)\n",
      "Test Accuracy of    식혜: 97.41% (976/1002)\n",
      "Test Accuracy of 쌀밥tmp: 90.27% (232/257)\n",
      "Test Accuracy of 아이스크림: 95.10% (951/1000)\n",
      "Test Accuracy of    알밥: 96.00% (960/1000)\n",
      "Test Accuracy of  애플파이: 95.60% (956/1000)\n",
      "Test Accuracy of 애호박볶음: 99.00% (990/1000)\n",
      "Test Accuracy of    약과: 99.10% (994/1003)\n",
      "Test Accuracy of    약식: 98.41% (988/1004)\n",
      "Test Accuracy of  양념게장: 98.50% (985/1000)\n",
      "Test Accuracy of  양념치킨: 96.50% (964/999)\n",
      "Test Accuracy of   양파링: 99.10% (991/1000)\n",
      "Test Accuracy of  어묵볶음: 97.80% (979/1001)\n",
      "Test Accuracy of  연근조림: 99.90% (999/1000)\n",
      "Test Accuracy of  연어요리: 96.10% (961/1000)\n",
      "Test Accuracy of  열무국수: 95.90% (958/999)\n",
      "Test Accuracy of  열무김치: 93.80% (938/1000)\n",
      "Test Accuracy of   오믈렛: 96.10% (961/1000)\n",
      "Test Accuracy of 오이소박이: 98.60% (986/1000)\n",
      "Test Accuracy of 오징어채볶음: 70.93% (766/1080)\n",
      "Test Accuracy of 오징어튀김: 91.92% (921/1002)\n",
      "Test Accuracy of    와플: 98.20% (982/1000)\n",
      "Test Accuracy of  우엉조림: 95.62% (960/1004)\n",
      "Test Accuracy of  유부초밥: 99.00% (990/1000)\n",
      "Test Accuracy of   육개장: 89.90% (899/1000)\n",
      "Test Accuracy of    육회: 97.70% (977/1000)\n",
      "Test Accuracy of  잔치국수: 98.20% (981/999)\n",
      "Test Accuracy of   잡곡밥: 95.20% (952/1000)\n",
      "Test Accuracy of    잡채: 97.90% (979/1000)\n",
      "Test Accuracy of  장어구이: 97.90% (978/999)\n",
      "Test Accuracy of   장조림: 97.11% (975/1004)\n",
      "Test Accuracy of   전복죽: 99.80% (998/1000)\n",
      "Test Accuracy of    젓갈: 96.90% (969/1000)\n",
      "Test Accuracy of  제육볶음: 96.79% (966/998)\n",
      "Test Accuracy of  조개구이: 99.30% (993/1000)\n",
      "Test Accuracy of  조기구이: 98.10% (981/1000)\n",
      "Test Accuracy of    족발: 94.41% (945/1001)\n",
      "Test Accuracy of 주꾸미볶음: 94.61% (948/1002)\n",
      "Test Accuracy of   주먹밥: 98.50% (985/1000)\n",
      "Test Accuracy of   짜장면: 99.40% (992/998)\n",
      "Test Accuracy of    짬뽕: 97.10% (971/1000)\n",
      "Test Accuracy of    쫄면: 96.80% (968/1000)\n",
      "Test Accuracy of   찐만두: 93.50% (935/1000)\n",
      "Test Accuracy of    찜닭: 96.61% (969/1003)\n",
      "Test Accuracy of    초밥: 96.20% (962/1000)\n",
      "Test Accuracy of 초콜릿케이크: 87.90% (879/1000)\n",
      "Test Accuracy of  총각김치: 97.49% (973/998)\n",
      "Test Accuracy of   추어탕: 92.32% (926/1003)\n",
      "Test Accuracy of   츄러스: 96.70% (967/1000)\n",
      "Test Accuracy of 치즈케이크: 88.40% (884/1000)\n",
      "Test Accuracy of   치킨윙: 97.40% (974/1000)\n",
      "Test Accuracy of  치킨카레: 93.90% (939/1000)\n",
      "Test Accuracy of   칼국수: 93.70% (937/1000)\n",
      "Test Accuracy of  컵케이크: 96.00% (960/1000)\n",
      "Test Accuracy of 코다리조림: 93.07% (1047/1125)\n",
      "Test Accuracy of   콩국수: 98.90% (989/1000)\n",
      "Test Accuracy of  콩나물국: 89.90% (899/1000)\n",
      "Test Accuracy of 콩나물무침: 98.40% (984/1000)\n",
      "Test Accuracy of   콩자반: 99.40% (994/1000)\n",
      "Test Accuracy of 크림스파게티: 97.50% (975/1000)\n",
      "Test Accuracy of    타꼬: 96.40% (964/1000)\n",
      "Test Accuracy of  타꼬야끼: 98.30% (983/1000)\n",
      "Test Accuracy of 탕수육tmp: 96.34% (395/410)\n",
      "Test Accuracy of 토마토스파게티: 98.90% (989/1000)\n",
      "Test Accuracy of   토스트: 96.10% (961/1000)\n",
      "Test Accuracy of  티라미슈: 98.70% (987/1000)\n",
      "Test Accuracy of   파김치: 98.10% (981/1000)\n",
      "Test Accuracy of    파전: 95.40% (954/1000)\n",
      "Test Accuracy of   팟타이: 97.40% (974/1000)\n",
      "Test Accuracy of  팬케이크: 94.40% (944/1000)\n",
      "Test Accuracy of    편육: 88.40% (884/1000)\n",
      "Test Accuracy of     포: 97.00% (970/1000)\n",
      "Test Accuracy of    피자: 97.70% (976/999)\n",
      "Test Accuracy of    한과: 99.21% (1002/1010)\n",
      "Test Accuracy of   핫도그: 98.20% (982/1000)\n",
      "Test Accuracy of   해물찜: 96.70% (968/1001)\n",
      "Test Accuracy of 해조류무침: 98.30% (983/1000)\n",
      "Test Accuracy of   햄버거: 95.30% (953/1000)\n",
      "Test Accuracy of 현미밥tmp: 90.42% (302/334)\n",
      "Test Accuracy of   호박전: 97.80% (977/999)\n",
      "Test Accuracy of   호박죽: 99.50% (995/1000)\n",
      "Test Accuracy of  홍어무침: 92.80% (928/1000)\n",
      "Test Accuracy of  황태구이: 96.40% (964/1000)\n",
      "Test Accuracy of     회: 96.80% (968/1000)\n",
      "Test Accuracy of   회무침: 79.18% (795/1004)\n",
      "Test Accuracy of 후라이드치킨: 96.50% (965/1000)\n",
      "Test Accuracy of  훈제오리: 94.90% (949/1000)\n",
      "\n",
      "Test Accuracy (Overall): 95.49% (188183/197076)\n"
     ]
    }
   ],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "    \n",
    "with open('D:/food/food-101/meta/class.txt', 'r',encoding='UTF8') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]\n",
    "#classes = ['apple_pie', 'baby_back_ribs', 'baklava']\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "batch_time = AverageMeter('Time', ':6.3f')\n",
    "losses = AverageMeter('Loss', ':.4e')\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "valid_acc_list = []\n",
    "#move model to gpu\n",
    "model1.cuda()\n",
    "model1.eval()\n",
    "\n",
    "# iterate over test data\n",
    "with torch.no_grad():\n",
    "  for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if USE_GPU:\n",
    "      data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "      ## For 10-crop Testing\n",
    "      bs, ncrops, c, h, w = data.size()\n",
    "      # forward pass: compute predicted outputs by passing inputs to the model\n",
    "      temp_output = model1(data.view(-1, c, h, w))\n",
    "      output = temp_output.view(bs, ncrops, -1).mean(1)\n",
    "      # calculate the batch loss\n",
    "      loss = criterion(output, target)\n",
    "      \n",
    "      acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "      #valid_acc_list.append(prec1.data/100)\n",
    "      \n",
    "\n",
    "    # update average test loss \n",
    "      test_loss += loss.item()*data.size(0)\n",
    "      # convert output probabilities to predicted class\n",
    "      top1.update(acc1[0], data.size(0))\n",
    "        \n",
    "      top5.update(acc5[0], data.size(0))\n",
    "    \n",
    "      _, pred = torch.max(output, 1)    \n",
    "      # compare predictions to true label\n",
    "      correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "      correct = np.squeeze(correct_tensor.numpy()) if not USE_GPU else np.squeeze(correct_tensor.cpu().numpy())\n",
    "      # calculate test accuracy for each object class\n",
    "      for i in range(len(target)):\n",
    "          label = target.data[i]\n",
    "          class_correct[label] += correct[i].item()\n",
    "          class_total[label] += 1\n",
    "            \n",
    "print(top5.avg)    \n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %.2f%% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],\n",
    "                                                         np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),\n",
    "                                                      np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Overall): 96.77%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy (Overall): 96.77%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = models.resnet101()            \n",
    "\n",
    "number_of_features =  model1.fc.in_features\n",
    "model1.fc = nn.Linear(number_of_features, 199)\n",
    "\n",
    "    \n",
    "model1.load_state_dict(torch.load(\"10_12_test_199_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "example_inputs = torch.rand(1, 3, 224, 224)\n",
    "resnet101_traced = torch.jit.trace(model1, example_inputs = example_inputs)\n",
    "resnet101_traced.save(\"resnet101_traced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6683e+00,  1.0028e+00,  1.3743e-01, -5.5724e-01,  2.1161e+00,\n",
      "          2.4716e-01, -1.6218e+00,  3.1941e+00, -3.6085e-01, -4.5259e-02,\n",
      "          1.6995e+00, -3.6884e-01, -4.1681e-01,  1.4955e+00,  1.5640e+00,\n",
      "          1.5341e+00,  1.6497e+00,  1.3310e+00,  2.2599e+00, -2.2023e-01,\n",
      "          4.1880e-01,  3.5987e-02, -1.5106e+00,  4.1830e+00, -3.9829e+00,\n",
      "         -3.6844e+00, -1.8753e+00, -2.0102e+00,  2.4859e+00,  3.0422e+00,\n",
      "          1.8782e+00,  1.4586e+00,  7.4914e-01,  1.4893e+00,  2.6295e-01,\n",
      "         -1.7380e+00, -1.9766e-01,  8.1723e-01,  1.7849e+00,  7.4463e-01,\n",
      "         -2.6215e+00,  4.9783e+00,  8.9715e-01,  1.0869e+00, -3.8777e-01,\n",
      "         -8.3069e-01,  9.8879e-01, -7.0240e-01,  2.6180e-01,  3.5164e+00,\n",
      "         -1.8721e+00, -1.7326e+00, -2.6319e+00, -2.6202e+00,  1.2930e+00,\n",
      "          3.2519e-01, -2.0535e+00,  3.2546e-01,  1.2196e+00, -1.1732e+00,\n",
      "         -4.4355e-01,  2.0602e+00,  1.9116e+00, -1.1772e+00, -5.4464e-01,\n",
      "          2.7470e+00, -1.5007e+00, -3.6143e-01, -1.1216e+00, -3.0593e+00,\n",
      "          1.1863e+00,  2.8421e-01, -5.3695e-01, -8.0921e-01, -3.6247e-02,\n",
      "          5.9191e-01,  1.0251e+00,  1.9461e+00,  9.2963e-01,  1.4490e+00,\n",
      "         -1.6367e+00,  1.6861e+00,  1.2989e+00, -3.5447e-02,  1.2655e+00,\n",
      "         -4.3318e-01,  9.0453e-01,  2.3003e+00,  1.3671e+00, -1.7332e+00,\n",
      "          1.6861e+00,  2.1163e+00, -1.1390e+00,  1.1720e+00,  3.4547e+00,\n",
      "         -7.0742e-01, -1.4176e+00, -2.8908e+00, -2.8126e+00,  8.0270e-01,\n",
      "         -8.8896e-01,  9.5870e-01, -1.6076e-01,  1.1492e+00,  1.9553e+00,\n",
      "          4.3749e-01, -5.7703e-02, -5.0324e-02, -2.0427e+00, -2.6523e+00,\n",
      "         -3.7587e-01,  6.7204e-01,  6.6058e-01,  7.3153e+00, -5.0683e-01,\n",
      "         -8.5218e-02, -2.1658e+00, -1.4480e+00,  2.1022e+00,  5.6546e-01,\n",
      "          3.3074e-01, -2.7136e+00,  1.1625e+00, -2.6276e+00, -9.2633e-02,\n",
      "          3.4783e-01, -1.8015e+00,  9.8377e-01,  7.5166e-01, -2.2871e+00,\n",
      "          1.3832e+00,  1.7933e+00, -1.6997e+00, -2.3917e+00,  2.2600e+00,\n",
      "         -1.3474e+00,  2.2270e+00, -2.9686e+00,  1.6592e+00,  2.3658e+00,\n",
      "          1.2342e+00, -3.5202e-01,  8.2332e-02, -1.0071e+00,  1.5000e+00,\n",
      "          1.1326e+00, -3.8642e+00, -2.3892e-01, -1.9481e+00, -2.6655e+00,\n",
      "         -9.7129e-01, -4.8303e-03,  1.2036e+00, -2.7882e+00, -2.4161e+00,\n",
      "         -1.6752e+00, -9.5050e-01, -2.6120e-01,  2.9511e-01,  6.6071e-01,\n",
      "         -1.6776e+00,  1.2872e+00, -2.3411e+00, -4.0032e+00,  1.9946e+00,\n",
      "          2.5836e-01,  8.1125e-01,  1.7434e+00,  2.3504e+00,  3.7144e-01,\n",
      "          1.0924e+00, -2.2924e+00, -2.1072e+00, -1.3924e+00, -2.0254e+00,\n",
      "         -1.5963e+00, -3.4719e+00, -5.8806e-01,  5.6599e-01,  1.3445e-01,\n",
      "         -3.6178e+00, -1.0569e-01,  2.6479e-01, -2.6809e+00,  1.7249e+00,\n",
      "         -1.8399e-01, -6.7304e-01, -8.7812e-01, -1.5079e+00, -1.3824e+00,\n",
      "          1.0526e+00,  6.4778e-01,  1.6833e+00, -1.2460e+00,  1.3219e+00,\n",
      "         -1.8140e+00, -1.2791e+00, -2.1098e-01,  7.0907e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model1(example_inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/food/food-101/meta/class.txt', 'r',encoding='UTF8') as txt:\n",
    "    classes = [l.strip() for l in txt.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "#class_names = [item[4:] for item in train_data.classes]\n",
    "\n",
    "def predict_food(img_path):\n",
    "    \n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "    # Resize\n",
    "    left_margin = (img.width-224)/2\n",
    "    bottom_margin = (img.height-224)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    img = img.crop((left_margin, bottom_margin, right_margin,   \n",
    "                      top_margin))\n",
    "    # Normalize\n",
    "    img = np.array(img)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406]) #provided mean\n",
    "    std = np.array([0.229, 0.224, 0.225]) #provided std\n",
    "    img = (img - mean)/std\n",
    "    \n",
    "    # Move color channels to first dimension as expected by PyTorch\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = torch.from_numpy(img).type(torch.cuda.FloatTensor) \n",
    "    img.unsqueeze_(0)\n",
    "    ps=torch.exp(model1(img))\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    \n",
    "    print(top_p)\n",
    "    \n",
    "    print(classes[top_class])\n",
    "    print(classes[top_class+1])\n",
    "    print(classes[top_class-1])\n",
    "    return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_food(img_path):\n",
    "    \n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "    # Resize\n",
    "#     left_margin = (img.width-224)/2\n",
    "#     bottom_margin = (img.height-224)/2\n",
    "#     right_margin = left_margin + 224\n",
    "#     top_margin = bottom_margin + 224\n",
    "#     img = img.crop((left_margin, bottom_margin, right_margin,   \n",
    "#                       top_margin))\n",
    "    # Normalize\n",
    "    img = np.array(img)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406]) #provided mean\n",
    "    std = np.array([0.229, 0.224, 0.225]) #provided std\n",
    "    img = (img - mean)/std\n",
    "    \n",
    "    # Move color channels to first dimension as expected by PyTorch\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = torch.from_numpy(img).type(torch.cuda.FloatTensor) \n",
    "    img.unsqueeze_(0)\n",
    "    ps=torch.exp(model1(img))\n",
    "    top_p, top_class = ps.topk(7, dim=1)\n",
    "    \n",
    "    \n",
    "    return top_class.data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "path_dir = 'D:/food/testdata/qw'\n",
    " \n",
    "file_list = os.listdir(path_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list1 = file_list[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list2= file_list[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list3= file_list[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list4= file_list[300:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list5= file_list[400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k , q in enumerate (file_list1):\n",
    "    pathtmp = path_dir + \"/\"+q\n",
    "    #print(pathtmp)\n",
    "    tmptt = predict_food(pathtmp)\n",
    "    \n",
    "    if classes[tmptt[0]] in q or classes[tmptt[1]] in q or classes[tmptt[2]] in q or classes[tmptt[3]] in q or classes[tmptt[4]] in q or classes[tmptt[5]] in q or classes[tmptt[6]] in q:\n",
    "        count = count+1\n",
    "    \n",
    "\n",
    "   \n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "80\n",
      "73\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k , q in enumerate (file_list2):\n",
    "    pathtmp = path_dir + \"/\"+q\n",
    "    #print(pathtmp)\n",
    "    tmptt = predict_food(pathtmp)\n",
    "    \n",
    "    if classes[tmptt[0]] in q or classes[tmptt[1]] in q or classes[tmptt[2]] in q or classes[tmptt[3]] in q or classes[tmptt[4]] in q or classes[tmptt[5]] in q or classes[tmptt[6]] in q:\n",
    "        count = count+1\n",
    "    \n",
    "\n",
    "   \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for k , q in enumerate (file_list3):\n",
    "    pathtmp = path_dir + \"/\"+q\n",
    "    #print(pathtmp)\n",
    "    tmptt = predict_food(pathtmp)\n",
    "    \n",
    "    if classes[tmptt[0]] in q or classes[tmptt[1]] in q or classes[tmptt[2]] in q or classes[tmptt[3]] in q or classes[tmptt[4]] in q or classes[tmptt[5]] in q or classes[tmptt[6]] in q:\n",
    "        count = count+1\n",
    "    \n",
    "\n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for k , q in enumerate (file_list4):\n",
    "    pathtmp = path_dir + \"/\"+q\n",
    "    #print(pathtmp)\n",
    "    tmptt = predict_food(pathtmp)\n",
    "    \n",
    "    if classes[tmptt[0]] in q or classes[tmptt[1]] in q or classes[tmptt[2]] in q or classes[tmptt[3]] in q or classes[tmptt[4]] in q or classes[tmptt[5]] in q or classes[tmptt[6]] in q:\n",
    "        count = count+1\n",
    "   \n",
    "print(count)\n",
    "\n",
    "count = 0\n",
    "for k , q in enumerate (file_list5):\n",
    "    pathtmp = path_dir + \"/\"+q\n",
    "    #print(pathtmp)\n",
    "    tmptt = predict_food(pathtmp)\n",
    "    \n",
    "    if classes[tmptt[0]] in q or classes[tmptt[1]] in q or classes[tmptt[2]] in q or classes[tmptt[3]] in q or classes[tmptt[4]] in q or classes[tmptt[5]] in q or classes[tmptt[6]] in q:\n",
    "        count = count+1\n",
    "    \n",
    "   \n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
